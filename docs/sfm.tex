Structure from motion is the process of estimating camera pose from a sequence of 2D images. The input to the structure from motion module is a sequence of images and the output is the rotation and translation matrices of the camera. The goal of our work is to make a structure from motion framework that works for a variety of different situations. The framework is able to process images alone, stereo pairs, and data from the Kinect. Another important aspect of the framework is that it is able to process large planetary images as well as quickly process rover images.

\subsection{Files:}
\begin{itemize}
	\item{tests/depthSfm\_test.cpp} - main function for SFM Processing
	\item{tests/FE\_test.cpp} - main function for testing FeatureExtraction class
	\item{SFM.h} - declaration of SFM class
	\item{SFM.cpp} - definition of SFM class
	\item{PoseEstimation.h} - declaration of PoseEstimation class
	\item{PoseEstimation.cpp} - definition of PoseEstimation class
	\item{FeatureExtraction.h} - declaration of FeatureExtraction class
	\item{FeatureExtraction.cpp} - definition of FeatureExtraction class
	\item{tests/testConfig.txt} - configuration file for main program
\end{itemize}


\subsection{How to install:}
\begin{enumerate}
	\item{Install Prerequisites} - Install OpenCV 2.3.1, PCL 1.5, lapack, cmake, gfortran
	\item{Install SFM} - inside sfm\_processing/tests directory type: cmake .
	\item{Build sfm\_test} - inside sfm\_processing/tests directory type: make
\end{enumerate}

\subsection{How to run:}
./sfmDepth\_test testConfig.txt testImageList.txt results/
./FE\_test 0 testImageList results/
\subsubsection{Configuration Parameters}
If the configuration file is not specified, the program will run with a set of default parameters.
\begin{itemize}
	\item{\textsc{firstFrame}} - first frame to process in the sequence
	\item{\textsc{lastFrame}} - last frame to process in the sequence
	\item{\textsc{frameStep}} - step between frames to process, for subsampling in time
	\item{\textsc{depthInfo}} - 0 (No Depth), 1 (Stereo Pairs), 2 (Kinect Depth), 3 (Stereo Point Clouds)
	\item{\textsc{showResultsFlag}} - 0 (Don't show), 1 (Show results)
	\item{\textsc{saveResultsFlag}} - 0 (Don't save), 1 (Save results)
	\item{\textsc{cameraCalibrationFilename}} - file containing camera calibration information for kinect
	\item{\textsc{pointCloudFilename}} - file containing filenames for all point cloud files
	\item{\textsc{kinectDepthFilename}} - file containing list of kinect depth filenames
	\item{\textsc{minMatches}} - minimum number of matches in order to do pose estimation
	\item{\textsc{detThresh}} - small threshold for determinant of R in order to do SVD
	\item{\textsc{featureMethod}} - 0 (SURF), 1 (ORB), 2 (SIFT)
	\item{\textsc{poseEstimationType}} - 0 (Pairs of frames for Pose Estimation), 1 (SBA)
	\item{\textsc{nndrRatio}} - ratio for determining good matches
	\item{\textsc{numNN}} - number of nearest neighbors to save for each key point
	\item{\textsc{zDist}} - maximum distance between z coordinates for good matches
	\item{\textsc{xDist}} - maximum distance between x coordinates for good matches
	\item{\textsc{yDist}} - maximum distance between y coordinates for good matches
	\item{\textsc{nbMatches}} - minimum number of matches for homography
	\item{\textsc{homographyMethod}} - 0 (Default OpenCV Method), 4 (Least Median Method), 8 (RANSAC Method)
	\item{\textsc{flannCheck}} - number of elements to visit in NN search
	\item{\textsc{ransacPixels}} - maximum number of pixels the match can be from the epipolar line
	\item{\textsc{ransacAccuracy}} - accuracy of homography before stopping iterations
	\item{\textsc{tileWidth}} - width of each tile in image, if set to -1, tile width is equal to image width
	\item{\textsc{tileHeight}} - height of each tile in image, if set to -1, tile height is equal to image height
	\item{\textsc{xOverlap}} - amount of overlap in the x direction between consecutive tiles
	\item{\textsc{yOverlap}} - amount of overlap in the y direction between consecutive tiles
	\item{\textsc{pointProjFilename}} - file where point projections are saved for later use by SBA
	\item{\textsc{weightedPortion}} - weights this portion of the image to the bottom of the image (work in progress \ldots)
	\item{\textsc{resultImageFilename}} - filename for saved result images
	\item{\textsc{hessianThresh}} - parameter for SURF
	\item{\textsc{nOctaves}} - parameter for SURF
	\item{\textsc{nOctaveLayers}} - parameter for SURF
	\item{\textsc{extended}} - parameter for SURF to extract extended features
	\item{\textsc{upright}} - parameter for SURF to extract non rotation invariant features
	\item{\textsc{octaves}} - parameter for SURF
	\item{\textsc{octaveLayers}} - parameter for SURF
	\item{\textsc{matchingMethod}} - 0 (Use NNDR), 1 (Marry Features)
	\item{\textsc{stereoLeftList}} - list of left stereo images
	\item{\textsc{stereoRightList}} - list of right stereo images
\end{itemize}


\subsection{Algorithm}

\subsubsection{Feature Extraction}
	We make use of OpenCV's feature extraction methods for this part of the algorithm. The structure from motion utilizes three different feature detection and extraction methods: Oriented Brief (ORB), Speeded Up Robust Features (SURF), and Scale Invariant Feature Transform (SIFT). All three methods have their advantages and disadvantages. Since this application is designed for use by NASA, the features must be extremely discriminatory and robust.
\begin{itemize} 
\item{ORB} \\
	ORB is extremely fast, but not always very robust. In many situations the rover won't have many object to extract features from, as it will mostly be a dirt surface that it is navigating. ORB is unable to detect robust features in an all dirt environment and therefore he matches that result from using ORB are not always very reliable.
\item{SURF} \\
	SURF is slower than ORB, but is able to detect many good features in a dirt-only environment. SURF easily doubled the number of keypoints detected per image and the matches that result from these features were much more robust than those from ORB.
\item{SIFT} \\
	SIFT is much slower than SURF, but is able to detect even more features than SURF in similar environments. Using SIFT results in really reliable matches as well, but since there are so many key points, matching takes longer than matching SURF features.
\item{Increasing Speed} \\
	Since the algorithm needs to run very quickly in order to be a part of rover software, we added a few options to speed up the Feature Extraction time. If 3D information is known, we apply a mask to the image, removing areas without any 3D information, before applying feature extraction. This cuts down the feature extraction time by half. Another option that is available for speed up is subsampling the image. Subsampling the image requires subsampling the set of 3D points and then applying the mask before feature extraction. The final feature extraction speed up option is to use upright features. This takes away the constraint that the features have to be rotationally invariant. This decreases time for both feature detection and extraction.
\end{itemize}

\subsubsection{Feature Matching}
	OpenCV's Fast Library for Approximate Nearest Neighbor (FLANN) is used for determining possible matches for the features extracted in the previous step. Feature points are extracted from a pair of images and FLANN is applied to both sets of feature points. We find the two nearest neighbors for each key point. From here, we have two matching options. 
\begin{itemize}
\item{Nearest Neighbor Distance Ratio} \\
	We determine if a potential match returned by FLANN is “good” by the Nearest Neighbor Distance Ratio (NNDR). If the ratio of distances of the reference key point's descriptor to its two nearest neighbors' descriptors is less than the NNDR, then the match is said to be “good” because it is much better than the second best match. 
\item{Marry Matches} \\
	The other matching method that we employ instead of the NNDR, is to marry matches from the reference image with those in the matching image. We find the nearest neighbor for each key point in the reference image and the nearest neighbor for each key point in the matching image. If a key point in the reference image has a nearest neighbor in the reference image whose nearest neighbor is that same key point in the reference image, these two key points are said to “marry.” In this case, this is considered a “good” match.
\end{itemize}

\subsubsection{Outlier Rejection}
There are two steps in the process of outlier rejection in our structure from motion framework.
\begin{itemize}
\item{Standard Deviation Constraint} \\
	First, for each match, we compute the translation between the two key points. If 3D information is available, either through Kinect or stereo data, then the 3D translation is computed for each point. If there is no 3D information available, in the case of image only input, then the 2D pixel translation is computed for each match. The mean and standard deviation of the translations of the entire match set are computed. The matches whose translation vector are outside of one standard deviation of the mean translation are rejected. 
\item{Homography} \\
	The standard deviation constraint is followed by another outlier removal technique. We use OpenCV to find a homography between the two images used to compute the matches. This homography tells us which of the matches that are left are outliers. These are removed, and the matches that survive this step are used to compute the camera pose in the next step. If there is no 3D information available, the rest of the algorithm is skipped.
\end{itemize}

\subsubsection{Relative Pose Estimation}
	This step involves computing the translation and rotation matrices that map the 3D points from the previous image to the current image.
\begin{itemize}
\item{Translation} \\
	The translation matrix is computed in a similar way to the way that it was for the outlier rejection. In this step, however, we use a weighted mean for the computation of the translation matrix. A parameter is set to weight the matches closer to the camera more than those further away from the camera. When the camera is moving, the change will be more noticeable closer to the camera, so those matches describe the movement of the camera more accurately.
$$T = m - R*r$$
Where $R$ is the rotation computed in the next step, $r$ is the reference key points, and $m$ is the matching key points.
\item{Rotation} \\
	The Kabsch algorithm is used to compute the relative rotation matrix for the camera. The Kabsch algorithm calculates the optimal rotation matrix that minimizes the root mean squared deviation between two paired sets of points. There are three steps in the Kabsch algorithm. First, both sets of 3D points must be translated so that their centroid is on the origin. Next, the covariance matrix for the data must be computed, and finally, the covariance matrix is decomposed using singular value decomposition. This decomposition is then used to compute the rotation matrix for the image pair. The relative rotation and translation matrices are used in the next step, where we compute the global pose of the camera.
$$A = (r-c_r)*(m-c_m)$$
Where $c_{r}$ is the center of the reference matches and $c_{m}$ is the center of the matching matches. We perform singular value decomposition on $A$ and use the resulting $U$, $W$, and $V$ matrices to compute $R$ as follows:
$$R = U*V$$
\end{itemize}

\subsubsection{Global Pose Estimation}
	The end goal of the structure from motion system is to estimate the pose of the camera at each frame with respect to the first frame. At the beginning of the image sequence, the camera has yet to move, so the translation and rotation are both zero. From this point, matches are computed between the first and second frame, resulting in relative rotation and translation matrices. Since the rotation and translation thus far are both zero, these relative rotation and translation matrices are now the global rotation and translation. For each pair of frames to follow, new relative rotation and translation matrices are computed. These relative matrices must now be applied to the global matrices to get the new global matrices. These new global rotation and translation matrices describe the pose of the camera at this particular frame with respect to the first frame.
$$T_g = T_p + T_c * R_p$$
$$R_g = R_p * R_c$$

Where $T_{g}$ is the new global translation, $T_{p}$ is the previous global translation, $T_{c}$ is the current relative translation, $R_{p}$ is the previous global rotation, $R_{c}$ is the current relative rotation, and $R_{g}$ is the new global rotation.

\subsubsection{3D Point Mapping}
	This step aligns the 3D point clouds from each frame of the image sequence with the first set of 3D points. In the previous step, we compute the rotation and translation from the first frame to the current frame. In this step, we need the rotation and translation that aligns the current frame with the first frame, so we must negate the translation matrix and transpose the rotation matrix. Once this is done, the new rotation and translation matrices are applied to each 3D point corresponding to the current frame.
\begin{displaymath}
\hat{m} \sim r = R^T * (m-T)
\end{displaymath}
Where $\hat{m}$ is the mapped matching point, $r$ is the reference point, $R$ is the global rotation matrix, $m$ is the matching point, and $T$ is the global translation matrix.


%=======
%\section{Installation of SFM}

%This is complete installation procedure for Mac OS X.

%\begin{lstlisting}
%- install prerequisites
%1) install Macport (http://www.macports.org/install.php) 
%2) install dependencies (http://pointclouds.org/downloads/macosx.html)
%3) install PCL (http://pointclouds.org/downloads/macosx.html)
%4) sudo port install opencv
%5) sudo port install cmake
%6) install gfortran (http://gcc.gnu.org/wiki/GFortranBinaries)
%7) install lapack(http://gcc.gnu.org/testing/testing-lapack.html)
%7.1) download lapack.tgz and unzip it
%7.2) rename make.inc.example to make.inc in the root directory of lapack 
%7.3) make blaslib && make
%8) install sba (http://www.ics.forth.gr/~lourakis/sba/) 
%8.1) edit demo/CMakeLists.txt

%# CMake file for sba's demo program

%INCLUDE_DIRECTORIES(..)
%LINK_DIRECTORIES(.. ${LAPACKBLAS_DIR})

%ADD_EXECUTABLE(eucsbademo eucsbademo.c imgproj.c readparams.c eucsbademo.h readparams.h)
%# libraries the demo depends on
%IF(HAVE_F2C)
%  TARGET_LINK_LIBRARIES(eucsbademo sba ${LAPACK_LIB} ${BLAS_LIB} ${F2C_LIB})
%ELSE(HAVE_F2C)
%  TARGET_LINK_LIBRARIES(eucsbademo sba ${LAPACK_LIB})
%ENDIF(HAVE_F2C)

%# make sure that the library is built before the demo
%ADD_DEPENDENCIES(eucsbademo sba)

%8.2) mkdir build && cd build
%8.3) ccmake ..
%8.3.1) turn HAVE_F2C OFF
%8.3.2) LAPACK_LIB = -framework vecLib
%8.4) make
%8.5) set PATH for the libsba.a

%For beginner, check opt/local if you install them with "sudo port install." Otherwise, usr/local

%- Initialize SFM
%1) svn co https://babelfish.arc.nasa.gov/svn/stereopipeline/sandbox/sfm/
%2) cd sfm
%3) mkdir build
%4) cd build
%5) cmake .. (here is big double dots)
%6) make -j2 && make install

%- Configure SFM
%1) cd sfm\build
%2) ccmake ..
%[Press 'c' to configure]
%[Select option 'BUILD_TESTS']
%[Press 'c' to reconfigure]
%[Press 'g' to generate]

%# Test SFM
%1) cd sfm/build
%2) make test
%3) view the file in sfm/build/Testing/Tempory/LastTest.log

%# Run SFM with an Example
%1) cd sfm/examples
%2) scp <username>@<machinename>:/irg/data/kinect/<directory> .
%3) cd <directory>
%4) cp ../sfm_config_example.txt sfm_config.txt
%5) cp ../camera_calibration_example.txt camera_calibration.txt
%6) open sfm_config.txt to change the value of inputDataName by <directory>
%7) sfm
%8) pc_vis <directory>.txt

%for example <directory>=2011_1202_bldg_269_2, <machinename>=pesto
%1) cd sfm/examples
%2) scp <username>@pesto:/irg/data/kinect/2011_1202_bldg_269_2 .
%3) cd 2011_1202_bldg_269_2
%4) cp ../sfm_config_example.txt sfm_config.txt
%5) cp ../camera_calibration_example.txt camera_calibration.txt
%6) open sfm_config.txt: inputDataName 2011_1202_bldg_269_2
%7) sfm
%8) pc_vis 2011_1202_bldg_269_2.txt

%# troubleshooting
%If any problem to link dependencies,
%Download PCL sources from http://pointclouds.org/downloads/ 
%- compile and install PCL
%1) cd PCL-<ver>-Source
%2) cmake .
%3) make
%4) make install

%Add the following to ~/.bash_profile file
%export DYLD_FALLBACK_LIBRARY_PATH=/usr/local/lib:/usr/lib
%\end{lstlisting}

%\end{document}
